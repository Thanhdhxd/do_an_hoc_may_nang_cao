{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34bb2c23",
   "metadata": {},
   "source": [
    "# 2. Mã Hóa Dữ Liệu Train/Test Đã Chia\n",
    "\n",
    "Notebook này mã hóa:\n",
    "- **Train set đã augment** từ `train_augmented.csv`\n",
    "- **Test set gốc (không augment)** từ `test_original.csv`\n",
    "\n",
    "**Quan trọng:** Vocabulary sẽ được xây dựng từ TRAIN set, sau đó áp dụng cho cả train và test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2cb05",
   "metadata": {},
   "source": [
    "## 2.1. Import Thư Viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd445da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã import thành công các thư viện!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "print('Đã import thành công các thư viện!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ca6cc",
   "metadata": {},
   "source": [
    "## 2.2. Cấu Hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ae3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "   Train (augmented): ./split_augmented_data\\train_augmented.csv\n",
      "   Test (original): ./split_augmented_data\\test_original.csv\n",
      "Output: ./encoded_split_data/\n",
      " Min word frequency: 2\n",
      " Max vocab size: 50000\n"
     ]
    }
   ],
   "source": [
    "# Đường dẫn input\n",
    "DATA_DIR = './split_augmented_data'\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, 'train_augmented.csv')\n",
    "TEST_PATH = os.path.join(DATA_DIR, 'test_original.csv')\n",
    "\n",
    "# Đường dẫn output\n",
    "OUTPUT_DIR = './encoded_split_data'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Tên cột\n",
    "TEXT_COLUMN = 'review'\n",
    "LABEL_COLUMN = 'sentiment'\n",
    "\n",
    "# Tham số encoding\n",
    "MIN_WORD_FREQ = 2  # Từ phải xuất hiện ít nhất 2 lần trong TRAIN\n",
    "MAX_VOCAB_SIZE = 50000  # Giới hạn kích thước vocabulary\n",
    "\n",
    "print(f'Input:')\n",
    "print(f'   Train (augmented): {TRAIN_PATH}')\n",
    "print(f'   Test (original): {TEST_PATH}')\n",
    "print(f'Output: {OUTPUT_DIR}/')\n",
    "print(f'Min word frequency: {MIN_WORD_FREQ}')\n",
    "print(f'Max vocab size: {MAX_VOCAB_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d9e50",
   "metadata": {},
   "source": [
    "## 2.3. Load Dữ Liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006adf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang load dữ liệu...\n",
      "\n",
      "Train set: 59,995 samples (đã augment)\n",
      "   Phân phối nhãn:\n",
      "sentiment\n",
      "negative    29997\n",
      "positive    29998\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set: 10,000 samples (KHÔNG augment)\n",
      "   Phân phối nhãn:\n",
      "sentiment\n",
      "negative    5000\n",
      "positive    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Đang load dữ liệu...\\n')\n",
    "\n",
    "# Load train (đã augment)\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "print(f'Train set: {len(train_df):,} samples (đã augment)')\n",
    "print(f'   Phân phối nhãn:')\n",
    "print(train_df[LABEL_COLUMN].value_counts().sort_index())\n",
    "\n",
    "# Load test (nguyên bản)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "print(f'\\nTest set: {len(test_df):,} samples (KHÔNG augment)')\n",
    "print(f'   Phân phối nhãn:')\n",
    "print(test_df[LABEL_COLUMN].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b58cc7",
   "metadata": {},
   "source": [
    "## 2.4. Tiền Xử Lý Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a5da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã định nghĩa hàm preprocess_text\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Tiền xử lý văn bản\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)  # Remove emails\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)  # Keep letters and numbers\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)  # Remove standalone numbers\n",
    "    text = ' '.join(text.split())  # Remove extra whitespace\n",
    "    \n",
    "    return text.split()\n",
    "\n",
    "print('Đã định nghĩa hàm preprocess_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3ca43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tiền xử lý texts...\n",
      "\n",
      "Train - Thống kê độ dài:\n",
      "   Min: 1, Max: 2485\n",
      "   Mean: 183.9, Median: 128\n",
      "   P95: 516\n",
      "\n",
      "Test - Thống kê độ dài:\n",
      "   Min: 6, Max: 2151\n",
      "   Mean: 233.2, Median: 175\n",
      "   P95: 597\n",
      "\n",
      "Sau khi loại bỏ samples rỗng:\n",
      "   Train: 59,995 samples\n",
      "   Test: 10,000 samples\n"
     ]
    }
   ],
   "source": [
    "# Tiền xử lý train và test\n",
    "print('Đang tiền xử lý texts...\\n')\n",
    "\n",
    "train_df['tokens'] = train_df[TEXT_COLUMN].apply(preprocess_text)\n",
    "test_df['tokens'] = test_df[TEXT_COLUMN].apply(preprocess_text)\n",
    "\n",
    "# Thống kê train\n",
    "train_lengths = train_df['tokens'].apply(len)\n",
    "print(f'Train - Thống kê độ dài:')\n",
    "print(f'   Min: {train_lengths.min()}, Max: {train_lengths.max()}')\n",
    "print(f'   Mean: {train_lengths.mean():.1f}, Median: {train_lengths.median():.0f}')\n",
    "print(f'   P95: {train_lengths.quantile(0.95):.0f}')\n",
    "\n",
    "# Thống kê test\n",
    "test_lengths = test_df['tokens'].apply(len)\n",
    "print(f'\\nTest - Thống kê độ dài:')\n",
    "print(f'   Min: {test_lengths.min()}, Max: {test_lengths.max()}')\n",
    "print(f'   Mean: {test_lengths.mean():.1f}, Median: {test_lengths.median():.0f}')\n",
    "print(f'   P95: {test_lengths.quantile(0.95):.0f}')\n",
    "\n",
    "# Loại bỏ samples rỗng\n",
    "train_df = train_df[train_df['tokens'].apply(len) > 0].reset_index(drop=True)\n",
    "test_df = test_df[test_df['tokens'].apply(len) > 0].reset_index(drop=True)\n",
    "\n",
    "print(f'\\nSau khi loại bỏ samples rỗng:')\n",
    "print(f'   Train: {len(train_df):,} samples')\n",
    "print(f'   Test: {len(test_df):,} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfde1182",
   "metadata": {},
   "source": [
    "## 2.5. Xây Dựng Vocabulary từ TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b27a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang xây dựng vocabulary từ TRAIN set...\n",
      "\n",
      "Tổng số từ unique trong TRAIN: 103,256\n",
      "Tổng số từ (bao gồm lặp): 11,033,048\n",
      "\n",
      "Sau khi filter (freq >= 2): 62,442\n",
      "Vocab size cuối cùng: 50,000\n",
      "\n",
      "Vocabulary:\n",
      "   Size: 50,002\n",
      "   Special tokens: <PAD> (0), <UNK> (1)\n",
      "\n",
      "Top 15 từ phổ biến nhất trong TRAIN:\n",
      "    1. the             (628,968 lần)\n",
      "    2. a               (308,037 lần)\n",
      "    3. and             (305,512 lần)\n",
      "    4. of              (271,542 lần)\n",
      "    5. to              (249,209 lần)\n",
      "    6. is              (196,985 lần)\n",
      "    7. it              (184,850 lần)\n",
      "    8. i               (177,146 lần)\n",
      "    9. in              (174,928 lần)\n",
      "   10. this            (146,350 lần)\n",
      "   11. that            (135,504 lần)\n",
      "   12. s               (119,945 lần)\n",
      "   13. was             (94,077 lần)\n",
      "   14. movie           (84,278 lần)\n",
      "   15. as              (84,038 lần)\n"
     ]
    }
   ],
   "source": [
    "# QUAN TRỌNG: Chỉ xây dựng vocabulary từ TRAIN set\n",
    "print('Đang xây dựng vocabulary từ TRAIN set...\\n')\n",
    "\n",
    "word_counts = Counter()\n",
    "for tokens in train_df['tokens']:\n",
    "    word_counts.update(tokens)\n",
    "\n",
    "print(f'Tổng số từ unique trong TRAIN: {len(word_counts):,}')\n",
    "print(f'Tổng số từ (bao gồm lặp): {sum(word_counts.values()):,}')\n",
    "\n",
    "# Lọc từ theo tần suất\n",
    "filtered_words = {word: count for word, count in word_counts.items() \n",
    "                 if count >= MIN_WORD_FREQ}\n",
    "\n",
    "print(f'\\nSau khi filter (freq >= {MIN_WORD_FREQ}): {len(filtered_words):,}')\n",
    "\n",
    "# Giới hạn vocab size\n",
    "if len(filtered_words) > MAX_VOCAB_SIZE:\n",
    "    most_common = sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)[:MAX_VOCAB_SIZE]\n",
    "    vocab_words = [word for word, _ in most_common]\n",
    "else:\n",
    "    vocab_words = list(filtered_words.keys())\n",
    "\n",
    "print(f'Vocab size cuối cùng: {len(vocab_words):,}')\n",
    "# Tạo word2idx mapping\n",
    "word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "for word in vocab_words:\n",
    "    word2idx[word] = len(word2idx)\n",
    "\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "print(f'\\nVocabulary:')\n",
    "print(f'   Size: {len(word2idx):,}')\n",
    "print(f'   Special tokens: <PAD> (0), <UNK> (1)')\n",
    "\n",
    "# Top words\n",
    "print(f'\\nTop 15 từ phổ biến nhất trong TRAIN:')\n",
    "for i, (word, count) in enumerate(sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:15], 1):\n",
    "    print(f'   {i:2d}. {word:15s} ({count:,} lần)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e42ad2",
   "metadata": {},
   "source": [
    "## 2.6. Mã Hóa Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d48cdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping:\n",
      "   negative -> 0 (Train: 29,997, Test: 5,000)\n",
      "   positive -> 1 (Train: 29,998, Test: 5,000)\n",
      "\n",
      "Đã mã hóa labels cho train và test\n"
     ]
    }
   ],
   "source": [
    "# Tạo label mapping từ TRAIN\n",
    "unique_labels = sorted(train_df[LABEL_COLUMN].unique())\n",
    "label2idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "idx2label = {idx: label for label, idx in label2idx.items()}\n",
    "\n",
    "print(f'Label mapping:')\n",
    "for label, idx in label2idx.items():\n",
    "    train_count = (train_df[LABEL_COLUMN] == label).sum()\n",
    "    test_count = (test_df[LABEL_COLUMN] == label).sum()\n",
    "    print(f'   {label} -> {idx} (Train: {train_count:,}, Test: {test_count:,})')\n",
    "\n",
    "# Mã hóa labels\n",
    "train_df['label_encoded'] = train_df[LABEL_COLUMN].map(label2idx)\n",
    "test_df['label_encoded'] = test_df[LABEL_COLUMN].map(label2idx)\n",
    "\n",
    "print(f'\\nĐã mã hóa labels cho train và test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c9c86",
   "metadata": {},
   "source": [
    "## 2.7. Mã Hóa Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33016f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang mã hóa texts...\n",
      "\n",
      "Đã mã hóa 59,995 train samples\n",
      "Đã mã hóa 10,000 test samples\n",
      "\n",
      "Thống kê UNK trong TEST set:\n",
      "   UNK tokens: 20,165 / 2,331,919\n",
      "   UNK rate: 0.86%\n",
      "   Tỷ lệ UNK thấp = vocabulary train tốt!\n"
     ]
    }
   ],
   "source": [
    "def encode_text(tokens, word2idx):\n",
    "    \"\"\"Mã hóa tokens thành indices\"\"\"\n",
    "    unk_idx = word2idx.get('<UNK>', 1)\n",
    "    return [word2idx.get(token, unk_idx) for token in tokens]\n",
    "\n",
    "print('Đang mã hóa texts...\\n')\n",
    "\n",
    "# Encode train\n",
    "train_df['encoded'] = train_df['tokens'].apply(lambda x: encode_text(x, word2idx))\n",
    "print(f'Đã mã hóa {len(train_df):,} train samples')\n",
    "# Encode test (sử dụng CÙNG vocabulary từ train)\n",
    "test_df['encoded'] = test_df['tokens'].apply(lambda x: encode_text(x, word2idx))\n",
    "print(f'Đã mã hóa {len(test_df):,} test samples')\n",
    "\n",
    "# Kiểm tra tỷ lệ UNK trong test\n",
    "test_unk_count = sum(1 for seq in test_df['encoded'] for idx in seq if idx == 1)\n",
    "test_total_tokens = sum(len(seq) for seq in test_df['encoded'])\n",
    "unk_rate = test_unk_count / test_total_tokens * 100\n",
    "\n",
    "print(f'\\nThống kê UNK trong TEST set:')\n",
    "print(f'   UNK tokens: {test_unk_count:,} / {test_total_tokens:,}')\n",
    "print(f'   UNK rate: {unk_rate:.2f}%')\n",
    "print(f'   Tỷ lệ UNK thấp = vocabulary train tốt!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b6183b",
   "metadata": {},
   "source": [
    "## 2.8. Kiểm Tra Encoding/Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d81771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing encoding/decoding...\n",
      "\n",
      "================================================================================\n",
      "TRAIN SAMPLE\n",
      "================================================================================\n",
      "Label: negative -> 0\n",
      "\n",
      "Original (first 150 chars):\n",
      "   ***May contain spoilers***<br /><br />I had very high expectations for this film, based on the trailer. I knew a bit about the real Ed Gein, so I figu...\n",
      "\n",
      "Encoded (368 tokens):\n",
      "   [212, 2661, 926, 9, 67, 53, 293, 1202, 18, 11, 19, 413, 23, 2, 1432, 9, 631, 3, 233, 41]...\n",
      "\n",
      "Decoded:\n",
      "   may contain spoilers i had very high expectations for this film based on the trailer i knew a bit about the real ed gein so i figured this was a mediu...\n",
      "\n",
      "================================================================================\n",
      "TEST SAMPLE\n",
      "================================================================================\n",
      "Label: negative -> 0\n",
      "\n",
      "Original (first 150 chars):\n",
      "   This is loosely based on the ideas of the original 80's hit . It's set in the modern day as we see a base in Afghanistan get destroyed by a UAV right ...\n",
      "\n",
      "Encoded (955 tokens):\n",
      "   [11, 7, 3535, 413, 23, 2, 996, 5, 2, 207, 13, 561, 8, 13, 272, 10, 2, 679, 250, 16]...\n",
      "\n",
      "Decoded:\n",
      "   this is loosely based on the ideas of the original s hit it s set in the modern day as we see a base in afghanistan get destroyed by a <UNK> right at ...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(sequence, idx2word):\n",
    "    \"\"\"Giải mã sequence\"\"\"\n",
    "    words = [idx2word.get(idx, '<UNK>') for idx in sequence]\n",
    "    return ' '.join(words)\n",
    "\n",
    "print('Testing encoding/decoding...\\n')\n",
    "\n",
    "# Test với train\n",
    "print('='*80)\n",
    "print('TRAIN SAMPLE')\n",
    "print('='*80)\n",
    "sample = train_df.sample(1, random_state=42).iloc[0]\n",
    "print(f\"Label: {sample[LABEL_COLUMN]} -> {sample['label_encoded']}\")\n",
    "print(f\"\\nOriginal (first 150 chars):\\n   {sample[TEXT_COLUMN][:150]}...\")\n",
    "print(f\"\\nEncoded ({len(sample['encoded'])} tokens):\\n   {sample['encoded'][:20]}...\")\n",
    "decoded = decode_sequence(sample['encoded'], idx2word)\n",
    "print(f\"\\nDecoded:\\n   {decoded[:150]}...\")\n",
    "# Test với test\n",
    "print('\\n' + '='*80)\n",
    "print('TEST SAMPLE')\n",
    "print('='*80)\n",
    "sample = test_df.sample(1, random_state=42).iloc[0]\n",
    "print(f\"Label: {sample[LABEL_COLUMN]} -> {sample['label_encoded']}\")\n",
    "print(f\"\\nOriginal (first 150 chars):\\n   {sample[TEXT_COLUMN][:150]}...\")\n",
    "print(f\"\\nEncoded ({len(sample['encoded'])} tokens):\\n   {sample['encoded'][:20]}...\")\n",
    "decoded = decode_sequence(sample['encoded'], idx2word)\n",
    "print(f\"\\nDecoded:\\n   {decoded[:150]}...\")\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0abee",
   "metadata": {},
   "source": [
    "## 2.9. Lưu Dữ Liệu Đã Mã Hóa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edabb672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang lưu dữ liệu đã mã hóa...\n",
      "\n",
      "Đã lưu train_encoded_texts.npy và train_encoded_labels.npy\n",
      "Đã lưu test_encoded_texts.npy và test_encoded_labels.npy\n",
      "Đã lưu word2idx.json\n",
      "Đã lưu idx2word.json\n",
      "Đã lưu label2idx.json\n",
      "Đã lưu idx2label.json\n"
     ]
    }
   ],
   "source": [
    "print('Đang lưu dữ liệu đã mã hóa...\\n')\n",
    "\n",
    "# Lưu train encoded\n",
    "train_texts = train_df['encoded'].values\n",
    "train_labels = train_df['label_encoded'].values\n",
    "np.save(os.path.join(OUTPUT_DIR, 'train_encoded_texts.npy'), train_texts, allow_pickle=True)\n",
    "np.save(os.path.join(OUTPUT_DIR, 'train_encoded_labels.npy'), train_labels)\n",
    "print(f'Đã lưu train_encoded_texts.npy và train_encoded_labels.npy')\n",
    "\n",
    "# Lưu test encoded\n",
    "test_texts = test_df['encoded'].values\n",
    "test_labels = test_df['label_encoded'].values\n",
    "np.save(os.path.join(OUTPUT_DIR, 'test_encoded_texts.npy'), test_texts, allow_pickle=True)\n",
    "np.save(os.path.join(OUTPUT_DIR, 'test_encoded_labels.npy'), test_labels)\n",
    "print(f'Đã lưu test_encoded_texts.npy và test_encoded_labels.npy')\n",
    "\n",
    "# Lưu mappings\n",
    "with open(os.path.join(OUTPUT_DIR, 'word2idx.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx, f, ensure_ascii=False, indent=2)\n",
    "print(f'Đã lưu word2idx.json')\n",
    "\n",
    "idx2word_str = {str(k): v for k, v in idx2word.items()}\n",
    "with open(os.path.join(OUTPUT_DIR, 'idx2word.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word_str, f, ensure_ascii=False, indent=2)\n",
    "print(f'Đã lưu idx2word.json')\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'label2idx.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(label2idx, f, ensure_ascii=False, indent=2)\n",
    "print(f'Đã lưu label2idx.json')\n",
    "\n",
    "idx2label_str = {str(k): v for k, v in idx2label.items()}\n",
    "with open(os.path.join(OUTPUT_DIR, 'idx2label.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2label_str, f, ensure_ascii=False, indent=2)\n",
    "print(f'Đã lưu idx2label.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7578db2",
   "metadata": {},
   "source": [
    "## 2.10. Lưu Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444d1bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Đã lưu metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Tạo metadata\n",
    "train_enc_lengths = train_df['encoded'].apply(len)\n",
    "test_enc_lengths = test_df['encoded'].apply(len)\n",
    "\n",
    "metadata = {\n",
    "    'dataset_info': {\n",
    "        'train_source': TRAIN_PATH,\n",
    "        'test_source': TEST_PATH,\n",
    "        'train_samples': len(train_df),\n",
    "        'test_samples': len(test_df),\n",
    "        'num_classes': len(label2idx),\n",
    "        'train_augmented': True,\n",
    "        'test_augmented': False\n",
    "    },\n",
    "    'vocab_stats': {\n",
    "        'vocab_size': len(word2idx),\n",
    "        'built_from': 'train_set_only',\n",
    "        'min_word_freq': MIN_WORD_FREQ,\n",
    "        'max_vocab_size': MAX_VOCAB_SIZE,\n",
    "        'test_unk_rate': float(unk_rate)\n",
    "    },\n",
    "    'train_encoding_stats': {\n",
    "        'min_length': int(train_enc_lengths.min()),\n",
    "        'max_length': int(train_enc_lengths.max()),\n",
    "        'mean_length': float(train_enc_lengths.mean()),\n",
    "        'median_length': float(train_enc_lengths.median()),\n",
    "        'p95_length': float(train_enc_lengths.quantile(0.95))\n",
    "    },\n",
    "    'test_encoding_stats': {\n",
    "        'min_length': int(test_enc_lengths.min()),\n",
    "        'max_length': int(test_enc_lengths.max()),\n",
    "        'mean_length': float(test_enc_lengths.mean()),\n",
    "        'median_length': float(test_enc_lengths.median()),\n",
    "        'p95_length': float(test_enc_lengths.quantile(0.95))\n",
    "    },\n",
    "    'special_tokens': {\n",
    "        'pad_token': '<PAD>',\n",
    "        'pad_idx': 0,\n",
    "        'unk_token': '<UNK>',\n",
    "        'unk_idx': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'metadata.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f'\\nĐã lưu metadata.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13860597",
   "metadata": {},
   "source": [
    "## 2.11. Tổng Kết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a25dfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HOÀN THÀNH MÃ HÓA DỮ LIỆU!\n",
      "================================================================================\n",
      "\n",
      "Tổng Kết:\n",
      "   Output directory: ./encoded_split_data/\n",
      "   Vocab size: 50,002 (xây dựng từ TRAIN)\n",
      "   Num classes: 2\n",
      "\n",
      "TRAIN SET (ĐÃ AUGMENT):\n",
      "   Samples: 59,995\n",
      "   Sequence length: min=1, max=2485, mean=183.9\n",
      "   Phân phối nhãn:\n",
      "      negative: 29,997 (50.0%)\n",
      "      positive: 29,998 (50.0%)\n",
      "\n",
      "TEST SET (KHÔNG AUGMENT):\n",
      "   Samples: 10,000\n",
      "   Sequence length: min=6, max=2151, mean=233.2\n",
      "   UNK rate: 0.86%\n",
      "   Phân phối nhãn:\n",
      "      negative: 5,000 (50.0%)\n",
      "      positive: 5,000 (50.0%)\n",
      "\n",
      "Files đã lưu trong ./encoded_split_data/:\n",
      "   - idx2label.json                 (     0.0 KB)\n",
      "   - idx2word.json                  (  1125.9 KB)\n",
      "   - label2idx.json                 (     0.0 KB)\n",
      "   - metadata.json                  (     0.9 KB)\n",
      "   - test_encoded_labels.npy        (    78.2 KB)\n",
      "   - test_encoded_texts.npy         (  5460.5 KB)\n",
      "   - train_encoded_labels.npy       (   468.8 KB)\n",
      "   - train_encoded_texts.npy        ( 25947.9 KB)\n",
      "   - word2idx.json                  (  1028.2 KB)\n",
      "\n",
      "LƯU Ý QUAN TRỌNG:\n",
      "   Vocabulary chỉ xây dựng từ TRAIN set\n",
      "   Test set sử dụng vocabulary này (có thể có UNK)\n",
      "   Train đã augment, Test KHÔNG augment\n",
      "   Đảm bảo tính khách quan khi đánh giá!\n",
      "\n",
      "Bước tiếp theo:\n",
      "   Chạy notebook 3_train_with_split.ipynb để train model\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('HOÀN THÀNH MÃ HÓA DỮ LIỆU!')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\nTổng Kết:')\n",
    "print(f'   Output directory: {OUTPUT_DIR}/')\n",
    "print(f'   Vocab size: {len(word2idx):,} (xây dựng từ TRAIN)')\n",
    "print(f'   Num classes: {len(label2idx)}')\n",
    "\n",
    "print(f'\\nTRAIN SET (ĐÃ AUGMENT):')\n",
    "print(f'   Samples: {len(train_df):,}')\n",
    "print(f'   Sequence length: min={train_enc_lengths.min()}, max={train_enc_lengths.max()}, mean={train_enc_lengths.mean():.1f}')\n",
    "print(f'   Phân phối nhãn:')\n",
    "for label, count in train_df[LABEL_COLUMN].value_counts().sort_index().items():\n",
    "    pct = count / len(train_df) * 100\n",
    "    print(f'      {label}: {count:,} ({pct:.1f}%)')\n",
    "\n",
    "print(f'\\nTEST SET (KHÔNG AUGMENT):')\n",
    "print(f'   Samples: {len(test_df):,}')\n",
    "print(f'   Sequence length: min={test_enc_lengths.min()}, max={test_enc_lengths.max()}, mean={test_enc_lengths.mean():.1f}')\n",
    "print(f'   UNK rate: {unk_rate:.2f}%')\n",
    "print(f'   Phân phối nhãn:')\n",
    "for label, count in test_df[LABEL_COLUMN].value_counts().sort_index().items():\n",
    "    pct = count / len(test_df) * 100\n",
    "    print(f'      {label}: {count:,} ({pct:.1f}%)')\n",
    "\n",
    "print(f'\\nFiles đã lưu trong {OUTPUT_DIR}/:') \n",
    "for file in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    file_path = os.path.join(OUTPUT_DIR, file)\n",
    "    size_kb = os.path.getsize(file_path) / 1024\n",
    "    print(f'   - {file:30s} ({size_kb:>8.1f} KB)')\n",
    "\n",
    "print(f'\\nLƯU Ý QUAN TRỌNG:')\n",
    "print(f'   Vocabulary chỉ xây dựng từ TRAIN set')\n",
    "print(f'   Test set sử dụng vocabulary này (có thể có UNK)')\n",
    "print(f'   Train đã augment, Test KHÔNG augment')\n",
    "print(f'   Đảm bảo tính khách quan khi đánh giá!')\n",
    "\n",
    "print('\\n' + '='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec8cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
