{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34bb2c23",
   "metadata": {},
   "source": [
    "# 2. M√£ H√≥a D·ªØ Li·ªáu Train/Test ƒê√£ Chia\n",
    "\n",
    "Notebook n√†y m√£ h√≥a:\n",
    "- **Train set ƒë√£ augment** t·ª´ `train_augmented.csv`\n",
    "- **Test set g·ªëc (kh√¥ng augment)** t·ª´ `test_original.csv`\n",
    "\n",
    "**Quan tr·ªçng:** Vocabulary s·∫Ω ƒë∆∞·ª£c x√¢y d·ª±ng t·ª´ TRAIN set, sau ƒë√≥ √°p d·ª•ng cho c·∫£ train v√† test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2cb05",
   "metadata": {},
   "source": [
    "## 2.1. Import Th∆∞ Vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd445da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ import th√†nh c√¥ng c√°c th∆∞ vi·ªán!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "print('ƒê√£ import th√†nh c√¥ng c√°c th∆∞ vi·ªán!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ca6cc",
   "metadata": {},
   "source": [
    "## 2.2. C·∫•u H√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ae3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Input:\n",
      "   Train (augmented): ./split_augmented_data\\train_augmented.csv\n",
      "   Test (original): ./split_augmented_data\\test_original.csv\n",
      "üìÅ Output: ./encoded_split_data/\n",
      "‚öôÔ∏è  Min word frequency: 2\n",
      "‚öôÔ∏è  Max vocab size: 50000\n"
     ]
    }
   ],
   "source": [
    "# ƒê∆∞·ªùng d·∫´n input\n",
    "DATA_DIR = './split_augmented_data'\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, 'train_augmented.csv')\n",
    "TEST_PATH = os.path.join(DATA_DIR, 'test_original.csv')\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n output\n",
    "OUTPUT_DIR = './encoded_split_data'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# T√™n c·ªôt\n",
    "TEXT_COLUMN = 'review'\n",
    "LABEL_COLUMN = 'sentiment'\n",
    "\n",
    "# Tham s·ªë encoding\n",
    "MIN_WORD_FREQ = 2  # T·ª´ ph·∫£i xu·∫•t hi·ªán √≠t nh·∫•t 2 l·∫ßn trong TRAIN\n",
    "MAX_VOCAB_SIZE = 50000  # Gi·ªõi h·∫°n k√≠ch th∆∞·ªõc vocabulary\n",
    "\n",
    "print(f'Input:')\n",
    "print(f'   Train (augmented): {TRAIN_PATH}')\n",
    "print(f'   Test (original): {TEST_PATH}')\n",
    "print(f'Output: {OUTPUT_DIR}/')\n",
    "print(f'Min word frequency: {MIN_WORD_FREQ}')\n",
    "print(f'Max vocab size: {MAX_VOCAB_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d9e50",
   "metadata": {},
   "source": [
    "## 2.3. Load D·ªØ Li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006adf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ƒêang load d·ªØ li·ªáu...\n",
      "\n",
      "‚úÖ Train set: 59,995 samples (ƒë√£ augment)\n",
      "   Ph√¢n ph·ªëi nh√£n:\n",
      "sentiment\n",
      "negative    29997\n",
      "positive    29998\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ Test set: 10,000 samples (KH√îNG augment)\n",
      "   Ph√¢n ph·ªëi nh√£n:\n",
      "sentiment\n",
      "negative    5000\n",
      "positive    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('ƒêang load d·ªØ li·ªáu...\\n')\n",
    "\n",
    "# Load train (ƒë√£ augment)\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "print(f'Train set: {len(train_df):,} samples (ƒë√£ augment)')\n",
    "print(f'   Ph√¢n ph·ªëi nh√£n:')\n",
    "print(train_df[LABEL_COLUMN].value_counts().sort_index())\n",
    "\n",
    "# Load test (nguy√™n b·∫£n)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "print(f'\\nTest set: {len(test_df):,} samples (KH√îNG augment)')\n",
    "print(f'   Ph√¢n ph·ªëi nh√£n:')\n",
    "print(test_df[LABEL_COLUMN].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b58cc7",
   "metadata": {},
   "source": [
    "## 2.4. Ti·ªÅn X·ª≠ L√Ω Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a5da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a h√†m preprocess_text\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)  # Remove emails\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)  # Keep letters and numbers\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)  # Remove standalone numbers\n",
    "    text = ' '.join(text.split())  # Remove extra whitespace\n",
    "    \n",
    "    return text.split()\n",
    "\n",
    "print('ƒê√£ ƒë·ªãnh nghƒ©a h√†m preprocess_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3ca43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ ƒêang ti·ªÅn x·ª≠ l√Ω texts...\n",
      "\n",
      "üìè Train - Th·ªëng k√™ ƒë·ªô d√†i:\n",
      "   Min: 1, Max: 2485\n",
      "   Mean: 183.9, Median: 128\n",
      "   P95: 516\n",
      "\n",
      "üìè Test - Th·ªëng k√™ ƒë·ªô d√†i:\n",
      "   Min: 6, Max: 2151\n",
      "   Mean: 233.2, Median: 175\n",
      "   P95: 597\n",
      "\n",
      "‚úÖ Sau khi lo·∫°i b·ªè samples r·ªóng:\n",
      "   Train: 59,995 samples\n",
      "   Test: 10,000 samples\n"
     ]
    }
   ],
   "source": [
    "# Ti·ªÅn x·ª≠ l√Ω train v√† test\n",
    "print('ƒêang ti·ªÅn x·ª≠ l√Ω texts...\\n')\n",
    "\n",
    "train_df['tokens'] = train_df[TEXT_COLUMN].apply(preprocess_text)\n",
    "test_df['tokens'] = test_df[TEXT_COLUMN].apply(preprocess_text)\n",
    "\n",
    "# Th·ªëng k√™ train\n",
    "train_lengths = train_df['tokens'].apply(len)\n",
    "print(f'Train - Th·ªëng k√™ ƒë·ªô d√†i:')\n",
    "print(f'   Min: {train_lengths.min()}, Max: {train_lengths.max()}')\n",
    "print(f'   Mean: {train_lengths.mean():.1f}, Median: {train_lengths.median():.0f}')\n",
    "print(f'   P95: {train_lengths.quantile(0.95):.0f}')\n",
    "\n",
    "# Th·ªëng k√™ test\n",
    "test_lengths = test_df['tokens'].apply(len)\n",
    "print(f'\\nTest - Th·ªëng k√™ ƒë·ªô d√†i:')\n",
    "print(f'   Min: {test_lengths.min()}, Max: {test_lengths.max()}')\n",
    "print(f'   Mean: {test_lengths.mean():.1f}, Median: {test_lengths.median():.0f}')\n",
    "print(f'   P95: {test_lengths.quantile(0.95):.0f}')\n",
    "\n",
    "# Lo·∫°i b·ªè samples r·ªóng\n",
    "train_df = train_df[train_df['tokens'].apply(len) > 0].reset_index(drop=True)\n",
    "test_df = test_df[test_df['tokens'].apply(len) > 0].reset_index(drop=True)\n",
    "\n",
    "print(f'\\nSau khi lo·∫°i b·ªè samples r·ªóng:')\n",
    "print(f'   Train: {len(train_df):,} samples')\n",
    "print(f'   Test: {len(test_df):,} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfde1182",
   "metadata": {},
   "source": [
    "## 2.5. X√¢y D·ª±ng Vocabulary t·ª´ TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b27a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ ƒêang x√¢y d·ª±ng vocabulary t·ª´ TRAIN set...\n",
      "\n",
      "üìä T·ªïng s·ªë t·ª´ unique trong TRAIN: 103,256\n",
      "üìä T·ªïng s·ªë t·ª´ (bao g·ªìm l·∫∑p): 11,033,048\n",
      "\n",
      "üìä Sau khi filter (freq >= 2): 62,442\n",
      "üìä Vocab size cu·ªëi c√πng: 50,000\n",
      "\n",
      "‚úÖ Vocabulary:\n",
      "   Size: 50,002\n",
      "   Special tokens: <PAD> (0), <UNK> (1)\n",
      "\n",
      "üìù Top 15 t·ª´ ph·ªï bi·∫øn nh·∫•t trong TRAIN:\n",
      "    1. the             (628,968 l·∫ßn)\n",
      "    2. a               (308,037 l·∫ßn)\n",
      "    3. and             (305,512 l·∫ßn)\n",
      "    4. of              (271,542 l·∫ßn)\n",
      "    5. to              (249,209 l·∫ßn)\n",
      "    6. is              (196,985 l·∫ßn)\n",
      "    7. it              (184,850 l·∫ßn)\n",
      "    8. i               (177,146 l·∫ßn)\n",
      "    9. in              (174,928 l·∫ßn)\n",
      "   10. this            (146,350 l·∫ßn)\n",
      "   11. that            (135,504 l·∫ßn)\n",
      "   12. s               (119,945 l·∫ßn)\n",
      "   13. was             (94,077 l·∫ßn)\n",
      "   14. movie           (84,278 l·∫ßn)\n",
      "   15. as              (84,038 l·∫ßn)\n"
     ]
    }
   ],
   "source": [
    "# QUAN TR·ªåNG: Ch·ªâ x√¢y d·ª±ng vocabulary t·ª´ TRAIN set\n",
    "print('ƒêang x√¢y d·ª±ng vocabulary t·ª´ TRAIN set...\\n')\n",
    "\n",
    "word_counts = Counter()\n",
    "for tokens in train_df['tokens']:\n",
    "    word_counts.update(tokens)\n",
    "\n",
    "print(f'T·ªïng s·ªë t·ª´ unique trong TRAIN: {len(word_counts):,}')\n",
    "print(f'T·ªïng s·ªë t·ª´ (bao g·ªìm l·∫∑p): {sum(word_counts.values()):,}')\n",
    "\n",
    "# L·ªçc t·ª´ theo t·∫ßn su·∫•t\n",
    "filtered_words = {word: count for word, count in word_counts.items() \n",
    "                 if count >= MIN_WORD_FREQ}\n",
    "\n",
    "print(f'\\nSau khi filter (freq >= {MIN_WORD_FREQ}): {len(filtered_words):,}')\n",
    "\n",
    "# Gi·ªõi h·∫°n vocab size\n",
    "if len(filtered_words) > MAX_VOCAB_SIZE:\n",
    "    most_common = sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)[:MAX_VOCAB_SIZE]\n",
    "    vocab_words = [word for word, _ in most_common]\n",
    "else:\n",
    "    vocab_words = list(filtered_words.keys())\n",
    "\n",
    "print(f'Vocab size cu·ªëi c√πng: {len(vocab_words):,}')\n",
    "# T·∫°o word2idx mapping\n",
    "word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "for word in vocab_words:\n",
    "    word2idx[word] = len(word2idx)\n",
    "\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "print(f'\\nVocabulary:')\n",
    "print(f'   Size: {len(word2idx):,}')\n",
    "print(f'   Special tokens: <PAD> (0), <UNK> (1)')\n",
    "\n",
    "# Top words\n",
    "print(f'\\nTop 15 t·ª´ ph·ªï bi·∫øn nh·∫•t trong TRAIN:')\n",
    "for i, (word, count) in enumerate(sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:15], 1):\n",
    "    print(f'   {i:2d}. {word:15s} ({count:,} l·∫ßn)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e42ad2",
   "metadata": {},
   "source": [
    "## 2.6. M√£ H√≥a Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d48cdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Label mapping:\n",
      "   negative -> 0 (Train: 29,997, Test: 5,000)\n",
      "   positive -> 1 (Train: 29,998, Test: 5,000)\n",
      "\n",
      "‚úÖ ƒê√£ m√£ h√≥a labels cho train v√† test\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o label mapping t·ª´ TRAIN\n",
    "unique_labels = sorted(train_df[LABEL_COLUMN].unique())\n",
    "label2idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "idx2label = {idx: label for label, idx in label2idx.items()}\n",
    "\n",
    "print(f'Label mapping:')\n",
    "for label, idx in label2idx.items():\n",
    "    train_count = (train_df[LABEL_COLUMN] == label).sum()\n",
    "    test_count = (test_df[LABEL_COLUMN] == label).sum()\n",
    "    print(f'   {label} -> {idx} (Train: {train_count:,}, Test: {test_count:,})')\n",
    "\n",
    "# M√£ h√≥a labels\n",
    "train_df['label_encoded'] = train_df[LABEL_COLUMN].map(label2idx)\n",
    "test_df['label_encoded'] = test_df[LABEL_COLUMN].map(label2idx)\n",
    "\n",
    "print(f'\\nƒê√£ m√£ h√≥a labels cho train v√† test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c9c86",
   "metadata": {},
   "source": [
    "## 2.7. M√£ H√≥a Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33016f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ ƒêang m√£ h√≥a texts...\n",
      "\n",
      "‚úÖ ƒê√£ m√£ h√≥a 59,995 train samples\n",
      "‚úÖ ƒê√£ m√£ h√≥a 10,000 test samples\n",
      "\n",
      "üìä Th·ªëng k√™ UNK trong TEST set:\n",
      "   UNK tokens: 20,165 / 2,331,919\n",
      "   UNK rate: 0.86%\n",
      "   üí° T·ª∑ l·ªá UNK th·∫•p = vocabulary train t·ªët!\n"
     ]
    }
   ],
   "source": [
    "def encode_text(tokens, word2idx):\n",
    "    \"\"\"M√£ h√≥a tokens th√†nh indices\"\"\"\n",
    "    unk_idx = word2idx.get('<UNK>', 1)\n",
    "    return [word2idx.get(token, unk_idx) for token in tokens]\n",
    "\n",
    "print('ƒêang m√£ h√≥a texts...\\n')\n",
    "\n",
    "# Encode train\n",
    "train_df['encoded'] = train_df['tokens'].apply(lambda x: encode_text(x, word2idx))\n",
    "print(f'ƒê√£ m√£ h√≥a {len(train_df):,} train samples')\n",
    "# Encode test (s·ª≠ d·ª•ng C√ôNG vocabulary t·ª´ train)\n",
    "test_df['encoded'] = test_df['tokens'].apply(lambda x: encode_text(x, word2idx))\n",
    "print(f'ƒê√£ m√£ h√≥a {len(test_df):,} test samples')\n",
    "\n",
    "# Ki·ªÉm tra t·ª∑ l·ªá UNK trong test\n",
    "test_unk_count = sum(1 for seq in test_df['encoded'] for idx in seq if idx == 1)\n",
    "test_total_tokens = sum(len(seq) for seq in test_df['encoded'])\n",
    "unk_rate = test_unk_count / test_total_tokens * 100\n",
    "\n",
    "print(f'\\nTh·ªëng k√™ UNK trong TEST set:')\n",
    "print(f'   UNK tokens: {test_unk_count:,} / {test_total_tokens:,}')\n",
    "print(f'   UNK rate: {unk_rate:.2f}%')\n",
    "print(f'   T·ª∑ l·ªá UNK th·∫•p = vocabulary train t·ªët!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b6183b",
   "metadata": {},
   "source": [
    "## 2.8. Ki·ªÉm Tra Encoding/Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d81771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing encoding/decoding...\n",
      "\n",
      "================================================================================\n",
      "TRAIN SAMPLE\n",
      "================================================================================\n",
      "Label: negative -> 0\n",
      "\n",
      "üìù Original (first 150 chars):\n",
      "   ***May contain spoilers***<br /><br />I had very high expectations for this film, based on the trailer. I knew a bit about the real Ed Gein, so I figu...\n",
      "\n",
      "üî¢ Encoded (368 tokens):\n",
      "   [212, 2661, 926, 9, 67, 53, 293, 1202, 18, 11, 19, 413, 23, 2, 1432, 9, 631, 3, 233, 41]...\n",
      "\n",
      "üîÑ Decoded:\n",
      "   may contain spoilers i had very high expectations for this film based on the trailer i knew a bit about the real ed gein so i figured this was a mediu...\n",
      "\n",
      "================================================================================\n",
      "TEST SAMPLE\n",
      "================================================================================\n",
      "Label: negative -> 0\n",
      "\n",
      "üìù Original (first 150 chars):\n",
      "   This is loosely based on the ideas of the original 80's hit . It's set in the modern day as we see a base in Afghanistan get destroyed by a UAV right ...\n",
      "\n",
      "üî¢ Encoded (955 tokens):\n",
      "   [11, 7, 3535, 413, 23, 2, 996, 5, 2, 207, 13, 561, 8, 13, 272, 10, 2, 679, 250, 16]...\n",
      "\n",
      "üîÑ Decoded:\n",
      "   this is loosely based on the ideas of the original s hit it s set in the modern day as we see a base in afghanistan get destroyed by a <UNK> right at ...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(sequence, idx2word):\n",
    "    \"\"\"Gi·∫£i m√£ sequence\"\"\"\n",
    "    words = [idx2word.get(idx, '<UNK>') for idx in sequence]\n",
    "    return ' '.join(words)\n",
    "\n",
    "print('Testing encoding/decoding...\\n')\n",
    "\n",
    "# Test v·ªõi train\n",
    "print('='*80)\n",
    "print('TRAIN SAMPLE')\n",
    "print('='*80)\n",
    "sample = train_df.sample(1, random_state=42).iloc[0]\n",
    "print(f\"Label: {sample[LABEL_COLUMN]} -> {sample['label_encoded']}\")\n",
    "print(f\"\\nOriginal (first 150 chars):\\n   {sample[TEXT_COLUMN][:150]}...\")\n",
    "print(f\"\\nEncoded ({len(sample['encoded'])} tokens):\\n   {sample['encoded'][:20]}...\")\n",
    "decoded = decode_sequence(sample['encoded'], idx2word)\n",
    "print(f\"\\nDecoded:\\n   {decoded[:150]}...\")\n",
    "# Test v·ªõi test\n",
    "print('\\n' + '='*80)\n",
    "print('TEST SAMPLE')\n",
    "print('='*80)\n",
    "sample = test_df.sample(1, random_state=42).iloc[0]\n",
    "print(f\"Label: {sample[LABEL_COLUMN]} -> {sample['label_encoded']}\")\n",
    "print(f\"\\nOriginal (first 150 chars):\\n   {sample[TEXT_COLUMN][:150]}...\")\n",
    "print(f\"\\nEncoded ({len(sample['encoded'])} tokens):\\n   {sample['encoded'][:20]}...\")\n",
    "decoded = decode_sequence(sample['encoded'], idx2word)\n",
    "print(f\"\\nDecoded:\\n   {decoded[:150]}...\")\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0abee",
   "metadata": {},
   "source": [
    "## 2.9. L∆∞u D·ªØ Li·ªáu ƒê√£ M√£ H√≥a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edabb672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ ƒêang l∆∞u d·ªØ li·ªáu ƒë√£ m√£ h√≥a...\n",
      "\n",
      "‚úÖ ƒê√£ l∆∞u train_encoded_texts.npy v√† train_encoded_labels.npy\n",
      "‚úÖ ƒê√£ l∆∞u test_encoded_texts.npy v√† test_encoded_labels.npy\n",
      "‚úÖ ƒê√£ l∆∞u word2idx.json\n",
      "‚úÖ ƒê√£ l∆∞u idx2word.json\n",
      "‚úÖ ƒê√£ l∆∞u label2idx.json\n",
      "‚úÖ ƒê√£ l∆∞u idx2label.json\n"
     ]
    }
   ],
   "source": [
    "print('ƒêang l∆∞u d·ªØ li·ªáu ƒë√£ m√£ h√≥a...\\n')\n",
    "\n",
    "# L∆∞u train encoded\n",
    "train_texts = train_df['encoded'].values\n",
    "train_labels = train_df['label_encoded'].values\n",
    "np.save(os.path.join(OUTPUT_DIR, 'train_encoded_texts.npy'), train_texts, allow_pickle=True)\n",
    "np.save(os.path.join(OUTPUT_DIR, 'train_encoded_labels.npy'), train_labels)\n",
    "print(f'ƒê√£ l∆∞u train_encoded_texts.npy v√† train_encoded_labels.npy')\n",
    "\n",
    "# L∆∞u test encoded\n",
    "test_texts = test_df['encoded'].values\n",
    "test_labels = test_df['label_encoded'].values\n",
    "np.save(os.path.join(OUTPUT_DIR, 'test_encoded_texts.npy'), test_texts, allow_pickle=True)\n",
    "np.save(os.path.join(OUTPUT_DIR, 'test_encoded_labels.npy'), test_labels)\n",
    "print(f'ƒê√£ l∆∞u test_encoded_texts.npy v√† test_encoded_labels.npy')\n",
    "\n",
    "# L∆∞u mappings\n",
    "with open(os.path.join(OUTPUT_DIR, 'word2idx.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx, f, ensure_ascii=False, indent=2)\n",
    "print(f'ƒê√£ l∆∞u word2idx.json')\n",
    "\n",
    "idx2word_str = {str(k): v for k, v in idx2word.items()}\n",
    "with open(os.path.join(OUTPUT_DIR, 'idx2word.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word_str, f, ensure_ascii=False, indent=2)\n",
    "print(f'ƒê√£ l∆∞u idx2word.json')\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'label2idx.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(label2idx, f, ensure_ascii=False, indent=2)\n",
    "print(f'ƒê√£ l∆∞u label2idx.json')\n",
    "\n",
    "idx2label_str = {str(k): v for k, v in idx2label.items()}\n",
    "with open(os.path.join(OUTPUT_DIR, 'idx2label.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2label_str, f, ensure_ascii=False, indent=2)\n",
    "print(f'ƒê√£ l∆∞u idx2label.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7578db2",
   "metadata": {},
   "source": [
    "## 2.10. L∆∞u Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444d1bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ ƒê√£ l∆∞u metadata.json\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o metadata\n",
    "train_enc_lengths = train_df['encoded'].apply(len)\n",
    "test_enc_lengths = test_df['encoded'].apply(len)\n",
    "\n",
    "metadata = {\n",
    "    'dataset_info': {\n",
    "        'train_source': TRAIN_PATH,\n",
    "        'test_source': TEST_PATH,\n",
    "        'train_samples': len(train_df),\n",
    "        'test_samples': len(test_df),\n",
    "        'num_classes': len(label2idx),\n",
    "        'train_augmented': True,\n",
    "        'test_augmented': False\n",
    "    },\n",
    "    'vocab_stats': {\n",
    "        'vocab_size': len(word2idx),\n",
    "        'built_from': 'train_set_only',\n",
    "        'min_word_freq': MIN_WORD_FREQ,\n",
    "        'max_vocab_size': MAX_VOCAB_SIZE,\n",
    "        'test_unk_rate': float(unk_rate)\n",
    "    },\n",
    "    'train_encoding_stats': {\n",
    "        'min_length': int(train_enc_lengths.min()),\n",
    "        'max_length': int(train_enc_lengths.max()),\n",
    "        'mean_length': float(train_enc_lengths.mean()),\n",
    "        'median_length': float(train_enc_lengths.median()),\n",
    "        'p95_length': float(train_enc_lengths.quantile(0.95))\n",
    "    },\n",
    "    'test_encoding_stats': {\n",
    "        'min_length': int(test_enc_lengths.min()),\n",
    "        'max_length': int(test_enc_lengths.max()),\n",
    "        'mean_length': float(test_enc_lengths.mean()),\n",
    "        'median_length': float(test_enc_lengths.median()),\n",
    "        'p95_length': float(test_enc_lengths.quantile(0.95))\n",
    "    },\n",
    "    'special_tokens': {\n",
    "        'pad_token': '<PAD>',\n",
    "        'pad_idx': 0,\n",
    "        'unk_token': '<UNK>',\n",
    "        'unk_idx': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'metadata.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f'\\nƒê√£ l∆∞u metadata.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13860597",
   "metadata": {},
   "source": [
    "## 2.11. T·ªïng K·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a25dfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéâ HO√ÄN TH√ÄNH M√É H√ìA D·ªÆ LI·ªÜU!\n",
      "================================================================================\n",
      "\n",
      "üìä T·ªïng K·∫øt:\n",
      "   ‚úÖ Output directory: ./encoded_split_data/\n",
      "   ‚úÖ Vocab size: 50,002 (x√¢y d·ª±ng t·ª´ TRAIN)\n",
      "   ‚úÖ Num classes: 2\n",
      "\n",
      "üîµ TRAIN SET (ƒê√É AUGMENT):\n",
      "   Samples: 59,995\n",
      "   Sequence length: min=1, max=2485, mean=183.9\n",
      "   Ph√¢n ph·ªëi nh√£n:\n",
      "      negative: 29,997 (50.0%)\n",
      "      positive: 29,998 (50.0%)\n",
      "\n",
      "üî¥ TEST SET (KH√îNG AUGMENT):\n",
      "   Samples: 10,000\n",
      "   Sequence length: min=6, max=2151, mean=233.2\n",
      "   UNK rate: 0.86%\n",
      "   Ph√¢n ph·ªëi nh√£n:\n",
      "      negative: 5,000 (50.0%)\n",
      "      positive: 5,000 (50.0%)\n",
      "\n",
      "üíæ Files ƒë√£ l∆∞u trong ./encoded_split_data/:\n",
      "   - idx2label.json                 (     0.0 KB)\n",
      "   - idx2word.json                  (  1125.9 KB)\n",
      "   - label2idx.json                 (     0.0 KB)\n",
      "   - metadata.json                  (     0.9 KB)\n",
      "   - test_encoded_labels.npy        (    78.2 KB)\n",
      "   - test_encoded_texts.npy         (  5460.5 KB)\n",
      "   - train_encoded_labels.npy       (   468.8 KB)\n",
      "   - train_encoded_texts.npy        ( 25947.9 KB)\n",
      "   - word2idx.json                  (  1028.2 KB)\n",
      "\n",
      "‚ö†Ô∏è  L∆ØU √ù QUAN TR·ªåNG:\n",
      "   ‚úÖ Vocabulary ch·ªâ x√¢y d·ª±ng t·ª´ TRAIN set\n",
      "   ‚úÖ Test set s·ª≠ d·ª•ng vocabulary n√†y (c√≥ th·ªÉ c√≥ UNK)\n",
      "   ‚úÖ Train ƒë√£ augment, Test KH√îNG augment\n",
      "   ‚úÖ ƒê·∫£m b·∫£o t√≠nh kh√°ch quan khi ƒë√°nh gi√°!\n",
      "\n",
      "üí° B∆∞·ªõc ti·∫øp theo:\n",
      "   Ch·∫°y notebook 3_train_with_split.ipynb ƒë·ªÉ train model\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('HO√ÄN TH√ÄNH M√É H√ìA D·ªÆ LI·ªÜU!')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\nT·ªïng K·∫øt:')\n",
    "print(f'   Output directory: {OUTPUT_DIR}/')\n",
    "print(f'   Vocab size: {len(word2idx):,} (x√¢y d·ª±ng t·ª´ TRAIN)')\n",
    "print(f'   Num classes: {len(label2idx)}')\n",
    "\n",
    "print(f'\\nTRAIN SET (ƒê√É AUGMENT):')\n",
    "print(f'   Samples: {len(train_df):,}')\n",
    "print(f'   Sequence length: min={train_enc_lengths.min()}, max={train_enc_lengths.max()}, mean={train_enc_lengths.mean():.1f}')\n",
    "print(f'   Ph√¢n ph·ªëi nh√£n:')\n",
    "for label, count in train_df[LABEL_COLUMN].value_counts().sort_index().items():\n",
    "    pct = count / len(train_df) * 100\n",
    "    print(f'      {label}: {count:,} ({pct:.1f}%)')\n",
    "\n",
    "print(f'\\nTEST SET (KH√îNG AUGMENT):')\n",
    "print(f'   Samples: {len(test_df):,}')\n",
    "print(f'   Sequence length: min={test_enc_lengths.min()}, max={test_enc_lengths.max()}, mean={test_enc_lengths.mean():.1f}')\n",
    "print(f'   UNK rate: {unk_rate:.2f}%')\n",
    "print(f'   Ph√¢n ph·ªëi nh√£n:')\n",
    "for label, count in test_df[LABEL_COLUMN].value_counts().sort_index().items():\n",
    "    pct = count / len(test_df) * 100\n",
    "    print(f'      {label}: {count:,} ({pct:.1f}%)')\n",
    "\n",
    "print(f'\\nFiles ƒë√£ l∆∞u trong {OUTPUT_DIR}/:') \n",
    "for file in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    file_path = os.path.join(OUTPUT_DIR, file)\n",
    "    size_kb = os.path.getsize(file_path) / 1024\n",
    "    print(f'   - {file:30s} ({size_kb:>8.1f} KB)')\n",
    "\n",
    "print(f'\\nL∆ØU √ù QUAN TR·ªåNG:')\n",
    "print(f'   Vocabulary ch·ªâ x√¢y d·ª±ng t·ª´ TRAIN set')\n",
    "print(f'   Test set s·ª≠ d·ª•ng vocabulary n√†y (c√≥ th·ªÉ c√≥ UNK)')\n",
    "print(f'   Train ƒë√£ augment, Test KH√îNG augment')\n",
    "print(f'   ƒê·∫£m b·∫£o t√≠nh kh√°ch quan khi ƒë√°nh gi√°!')\n",
    "\n",
    "print('\\n' + '='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec8cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
