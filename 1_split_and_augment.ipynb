{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cacab47f",
   "metadata": {},
   "source": [
    "# 1. Chia Train/Test vÃ  Augment Dá»¯ Liá»‡u (Back Translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14d1cdc",
   "metadata": {},
   "source": [
    "## 1.1. Import ThÆ° Viá»‡n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190228ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ import thÃ nh cÃ´ng cÃ¡c thÆ° viá»‡n!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from googletrans import Translator\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Set seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "print('ÄÃ£ import thÃ nh cÃ´ng cÃ¡c thÆ° viá»‡n!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f96887",
   "metadata": {},
   "source": [
    "## 1.2. Cáº¥u HÃ¬nh Tham Sá»‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2931307f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Input: ./data/dataset.csv\n",
      "ğŸ“ Output directory: ./split_augmented_data/\n",
      "âš™ï¸  Test size: 0.2 (train size: 0.8)\n",
      "âš™ï¸  Augment ratio (train only): 0.5\n",
      "âš™ï¸  Back Translation - Languages: ['de', 'fr'], Max length: 500\n",
      "âš™ï¸  Delay between requests: 0.5s\n",
      "\n",
      "ğŸ’¡ CHá»ˆ sá»­ dá»¥ng Back Translation Ä‘á»ƒ augment dá»¯ liá»‡u!\n"
     ]
    }
   ],
   "source": [
    "# ÄÆ°á»ng dáº«n\n",
    "DATA_PATH = './data/dataset.csv'\n",
    "OUTPUT_DIR = './split_augmented_data'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Tham sá»‘ chia dá»¯ liá»‡u\n",
    "TEST_SIZE = 0.2  # 20% cho test, 80% cho train\n",
    "\n",
    "# Tham sá»‘ augmentation (CHá»ˆ ÃP Dá»¤NG CHO TRAIN)\n",
    "AUGMENT_RATIO = 0.5  # TÄƒng 50% dá»¯ liá»‡u train\n",
    "\n",
    "# Back translation params\n",
    "INTERMEDIATE_LANGS = ['de', 'fr']  # DÃ¹ng nhiá»u ngÃ´n ngá»¯ Ä‘á»ƒ Ä‘a dáº¡ng hÆ¡n\n",
    "BACKTRANS_DELAY = 0.5  # Delay giá»¯a cÃ¡c request Ä‘á»ƒ trÃ¡nh rate limit\n",
    "MAX_BACKTRANS_LENGTH = 500  # Äá»™ dÃ i tá»‘i Ä‘a cá»§a text Ä‘á»ƒ back translate\n",
    "\n",
    "print(f'Input: {DATA_PATH}')\n",
    "print(f'Output directory: {OUTPUT_DIR}/')\n",
    "print(f' Test size: {TEST_SIZE} (train size: {1-TEST_SIZE})')\n",
    "print(f' Augment ratio (train only): {AUGMENT_RATIO}')\n",
    "print(f' Back Translation - Languages: {INTERMEDIATE_LANGS}, Max length: {MAX_BACKTRANS_LENGTH}')\n",
    "print(f' Delay between requests: {BACKTRANS_DELAY}s')\n",
    "print(f'\\nCHá»ˆ sá»­ dá»¥ng Back Translation Ä‘á»ƒ augment dá»¯ liá»‡u!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dc249b",
   "metadata": {},
   "source": [
    "## 1.3. Load vÃ  Chia Dá»¯ Liá»‡u Gá»‘c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e3be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Äang load dá»¯ liá»‡u gá»‘c...\n",
      "\n",
      "âœ… ÄÃ£ load 50,000 samples\n",
      "\n",
      "ğŸ“Š PhÃ¢n phá»‘i nhÃ£n gá»‘c:\n",
      "sentiment\n",
      "negative    25000\n",
      "positive    25000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ”„ Äang chia dá»¯ liá»‡u train/test (80%/20%)...\n",
      "\n",
      "âœ… ÄÃ£ chia dá»¯ liá»‡u:\n",
      "   Train: 40,000 samples\n",
      "   Test: 10,000 samples\n",
      "\n",
      "ğŸ“Š PhÃ¢n phá»‘i nhÃ£n train:\n",
      "sentiment\n",
      "negative    20000\n",
      "positive    20000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“Š PhÃ¢n phá»‘i nhÃ£n test:\n",
      "sentiment\n",
      "negative    5000\n",
      "positive    5000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ… ÄÃ£ load 50,000 samples\n",
      "\n",
      "ğŸ“Š PhÃ¢n phá»‘i nhÃ£n gá»‘c:\n",
      "sentiment\n",
      "negative    25000\n",
      "positive    25000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ”„ Äang chia dá»¯ liá»‡u train/test (80%/20%)...\n",
      "\n",
      "âœ… ÄÃ£ chia dá»¯ liá»‡u:\n",
      "   Train: 40,000 samples\n",
      "   Test: 10,000 samples\n",
      "\n",
      "ğŸ“Š PhÃ¢n phá»‘i nhÃ£n train:\n",
      "sentiment\n",
      "negative    20000\n",
      "positive    20000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“Š PhÃ¢n phá»‘i nhÃ£n test:\n",
      "sentiment\n",
      "negative    5000\n",
      "positive    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Äang load dá»¯ liá»‡u gá»‘c...')\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f'\\nÄÃ£ load {len(df):,} samples')\n",
    "print(f'\\nPhÃ¢n phá»‘i nhÃ£n gá»‘c:')\n",
    "print(df['sentiment'].value_counts().sort_index())\n",
    "\n",
    "# Chia train/test - QUAN TRá»ŒNG: Stratify Ä‘á»ƒ giá»¯ cÃ¢n báº±ng nhÃ£n\n",
    "print(f'\\nÄang chia dá»¯ liá»‡u train/test ({100*(1-TEST_SIZE):.0f}%/{100*TEST_SIZE:.0f}%)...')\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED,\n",
    "    stratify=df['sentiment']  # Giá»¯ cÃ¢n báº±ng nhÃ£n\n",
    ")\n",
    "\n",
    "print(f'\\nÄÃ£ chia dá»¯ liá»‡u:')\n",
    "print(f'   Train: {len(train_df):,} samples')\n",
    "print(f'   Test: {len(test_df):,} samples')\n",
    "\n",
    "print(f'\\nPhÃ¢n phá»‘i nhÃ£n train:')\n",
    "print(train_df['sentiment'].value_counts().sort_index())\n",
    "\n",
    "print(f'\\nPhÃ¢n phá»‘i nhÃ£n test:')\n",
    "print(test_df['sentiment'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af5e3c3",
   "metadata": {},
   "source": [
    "## 1.4. LÆ°u Test Set (KHÃ”NG AUGMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b38b146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ lÆ°u test set (KHÃ”NG augment): ./split_augmented_data\\test_original.csv\n",
      "   Sá»‘ samples: 10,000\n",
      "   ğŸ’¡ Test set nÃ y sáº½ KHÃ”NG bao giá» Ä‘Æ°á»£c augment!\n"
     ]
    }
   ],
   "source": [
    "# LÆ°u test set nguyÃªn báº£n - KHÃ”NG ÄÆ¯á»¢C AUGMENT\n",
    "test_path = os.path.join(OUTPUT_DIR, 'test_original.csv')\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(f'ÄÃ£ lÆ°u test set (KHÃ”NG augment): {test_path}')\n",
    "print(f'   Sá»‘ samples: {len(test_df):,}')\n",
    "print(f'   ğŸ’¡ Test set nÃ y sáº½ KHÃ”NG bao giá» Ä‘Æ°á»£c augment!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fd7cad",
   "metadata": {},
   "source": [
    "## 1.5. Äá»‹nh NghÄ©a Back Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a8ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ Ä‘á»‹nh nghÄ©a BackTranslator class\n"
     ]
    }
   ],
   "source": [
    "class BackTranslator:\n",
    "    \"\"\"\n",
    "    Back translation: English â†’ Intermediate Language â†’ English\n",
    "    Táº¡o phiÃªn báº£n paraphrase giá»¯ nguyÃªn Ã½ nghÄ©a\n",
    "    CHá»ˆ Sá»¬ Dá»¤NG CHO Táº¬P TRAIN!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, intermediate_langs=['de', 'fr', 'es']):\n",
    "        self.translator = Translator()\n",
    "        self.intermediate_langs = intermediate_langs\n",
    "        print(f'BackTranslator initialized')\n",
    "        print(f'   Intermediate languages: {intermediate_langs}')\n",
    "    \n",
    "    def back_translate(self, text, intermediate_lang=None, max_retries=3):\n",
    "        \"\"\"\n",
    "        Back translate text: en â†’ intermediate_lang â†’ en\n",
    "        \n",
    "        Args:\n",
    "            text: Text gá»‘c (English)\n",
    "            intermediate_lang: NgÃ´n ngá»¯ trung gian (náº¿u None thÃ¬ chá»n random)\n",
    "            max_retries: Sá»‘ láº§n retry náº¿u lá»—i\n",
    "        \n",
    "        Returns:\n",
    "            Back-translated text hoáº·c text gá»‘c náº¿u lá»—i\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return text\n",
    "        \n",
    "        # Chá»n ngÃ´n ngá»¯ trung gian\n",
    "        if intermediate_lang is None:\n",
    "            intermediate_lang = random.choice(self.intermediate_langs)\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Step 1: English â†’ Intermediate Language\n",
    "                translated = self.translator.translate(\n",
    "                    text, \n",
    "                    src='en', \n",
    "                    dest=intermediate_lang\n",
    "                )\n",
    "                intermediate_text = translated.text\n",
    "                \n",
    "                # Small delay\n",
    "                time.sleep(0.2)\n",
    "                \n",
    "                # Step 2: Intermediate Language â†’ English\n",
    "                back_translated = self.translator.translate(\n",
    "                    intermediate_text,\n",
    "                    src=intermediate_lang,\n",
    "                    dest='en'\n",
    "                )\n",
    "                result = back_translated.text\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    # Wait longer before retry\n",
    "                    time.sleep(2 ** attempt)\n",
    "                    continue\n",
    "                else:\n",
    "                    # Return original text if all retries fail\n",
    "                    return text\n",
    "        \n",
    "        return text\n",
    "\n",
    "print('ÄÃ£ Ä‘á»‹nh nghÄ©a BackTranslator class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c75d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BackTranslator initialized\n",
      "   Intermediate languages: ['de', 'fr']\n",
      "âœ… Sáºµn sÃ ng Ä‘á»ƒ augment TRAIN SET vá»›i Back Translation!\n"
     ]
    }
   ],
   "source": [
    "back_translator = BackTranslator(intermediate_langs=INTERMEDIATE_LANGS)\n",
    "print('Sáºµn sÃ ng Ä‘á»ƒ augment TRAIN SET vá»›i Back Translation!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5f8658",
   "metadata": {},
   "source": [
    "## 1.7. Test Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7fefa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing augmentation vá»›i máº«u tá»« TRAIN set...\n",
      "\n",
      "================================================================================\n",
      "Sentiment: negative\n",
      "\n",
      "ğŸ“ Original:\n",
      "   Why, oh why, is this trash considered a classic? I've seen higher body counts on episodes of The Simpsons. Virtually nothing happens in this film and ...\n",
      "\n",
      "ğŸŒ Back-translated:\n",
      "   Why, oh why, is this garbage considered a classic? I've seen a higher body count in The Simpsons episodes. Virtually nothing happens in this movie and...\n",
      "\n",
      "================================================================================\n",
      "Sentiment: negative\n",
      "\n",
      "ğŸ“ Original:\n",
      "   Why, oh why, is this trash considered a classic? I've seen higher body counts on episodes of The Simpsons. Virtually nothing happens in this film and ...\n",
      "\n",
      "ğŸŒ Back-translated:\n",
      "   Why, oh why, is this garbage considered a classic? I've seen a higher body count in The Simpsons episodes. Virtually nothing happens in this movie and...\n",
      "\n",
      "================================================================================\n",
      "Sentiment: negative\n",
      "\n",
      "ğŸ“ Original:\n",
      "   First off, Mexican Werewolf in Texas' title is misleading as many others have pointed out. It is actually about El Chupacabra, which is a similar crea...\n",
      "\n",
      "ğŸŒ Back-translated:\n",
      "   First of all, the title of \"Mexican Werewolf in Texas\" is misleading, as many others have already pointed out. It's actually about El Chupacabra, a cr...\n",
      "\n",
      "================================================================================\n",
      "Sentiment: negative\n",
      "\n",
      "ğŸ“ Original:\n",
      "   First off, Mexican Werewolf in Texas' title is misleading as many others have pointed out. It is actually about El Chupacabra, which is a similar crea...\n",
      "\n",
      "ğŸŒ Back-translated:\n",
      "   First of all, the title of \"Mexican Werewolf in Texas\" is misleading, as many others have already pointed out. It's actually about El Chupacabra, a cr...\n",
      "\n",
      "================================================================================\n",
      "Sentiment: positive\n",
      "\n",
      "ğŸ“ Original:\n",
      "   I rate this 10 out of 10. Why?<br /><br />* It offers insight into something I barely understand - the surfers surf because it's all they want to do; ...\n",
      "\n",
      "ğŸŒ Back-translated:\n",
      "   I rate this 10 out of 10. Why?<br /><br />* This gives insight into something I barely understand: Surfers surf because that's all they want to do; No...\n",
      "\n",
      "================================================================================\n",
      "Sentiment: positive\n",
      "\n",
      "ğŸ“ Original:\n",
      "   I rate this 10 out of 10. Why?<br /><br />* It offers insight into something I barely understand - the surfers surf because it's all they want to do; ...\n",
      "\n",
      "ğŸŒ Back-translated:\n",
      "   I rate this 10 out of 10. Why?<br /><br />* This gives insight into something I barely understand: Surfers surf because that's all they want to do; No...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test vá»›i 3 samples tá»« TRAIN set\n",
    "print('Testing augmentation vá»›i máº«u tá»« TRAIN set...\\n')\n",
    "test_samples = train_df.sample(3, random_state=42)\n",
    "\n",
    "for idx, row in test_samples.iterrows():\n",
    "    original = row['review']\n",
    "    \n",
    "    # Test back translation\n",
    "    truncated = original[:MAX_BACKTRANS_LENGTH] if len(original) > MAX_BACKTRANS_LENGTH else original\n",
    "    bt_augmented = back_translator.back_translate(truncated)\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Sentiment: {row['sentiment']}\")\n",
    "    print(f\"\\nOriginal:\")\n",
    "    print(f\"   {original[:150]}...\" if len(original) > 150 else f\"   {original}\")\n",
    "    print(f\"\\nBack-translated:\")\n",
    "    print(f\"   {bt_augmented[:150]}...\" if len(bt_augmented) > 150 else f\"   {bt_augmented}\")\n",
    "    print()\n",
    "    \n",
    "    time.sleep(BACKTRANS_DELAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb20a2",
   "metadata": {},
   "source": [
    "## 1.8. Augment CHá»ˆ Táº­p TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d0fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸš€ CHUáº¨N Bá»Š AUGMENT Táº¬P TRAIN (BACK TRANSLATION)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Sá»‘ samples cáº§n augment cho TRAIN set:\n",
      "   Sentiment negative: 10,000 samples\n",
      "   Sentiment positive: 10,000 samples\n",
      "\n",
      "â±ï¸  Thá»i gian Æ°á»›c tÃ­nh:\n",
      "   Back Translation: ~1500.0 phÃºt (20,000 samples)\n",
      "\n",
      "â±ï¸  Thá»i gian Æ°á»›c tÃ­nh CHO Má»–I SENTIMENT:\n",
      "   Sentiment negative: ~750.0 phÃºt\n",
      "   Sentiment positive: ~750.0 phÃºt\n",
      "\n",
      "ğŸ’¡ Má»—i sentiment cÃ³ thá»ƒ cháº¡y riÃªng trong cÃ¡c cell khÃ¡c nhau!\n",
      "\n",
      "âœ… ÄÃ£ khá»Ÿi táº¡o augmented_train_data = []\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print('CHUáº¨N Bá»Š AUGMENT Táº¬P TRAIN (BACK TRANSLATION)')\n",
    "print('='*80)\n",
    "\n",
    "# TÃ­nh sá»‘ samples cáº§n augment cho má»—i class trong TRAIN set\n",
    "train_samples_per_class = train_df['sentiment'].value_counts().to_dict()\n",
    "augment_per_class = {label: int(count * AUGMENT_RATIO) \n",
    "                     for label, count in train_samples_per_class.items()}\n",
    "\n",
    "print(f'\\nSá»‘ samples cáº§n augment cho TRAIN set:')\n",
    "total_augment = sum(augment_per_class.values())\n",
    "for label in sorted(train_df['sentiment'].unique()):\n",
    "    print(f'   Sentiment {label}: {augment_per_class[label]:,} samples')\n",
    "\n",
    "# Æ¯á»›c tÃ­nh thá»i gian (back translation)\n",
    "bt_time_est = total_augment * (4 + BACKTRANS_DELAY)  # ~4s per sample + delay\n",
    "total_time_est = bt_time_est / 60\n",
    "\n",
    "print(f'\\n Thá»i gian Æ°á»›c tÃ­nh:')\n",
    "print(f'   Back Translation: ~{total_time_est:.1f} phÃºt ({total_augment:,} samples)')\n",
    "\n",
    "# Æ¯á»›c tÃ­nh thá»i gian cho Má»–I SENTIMENT\n",
    "print(f'\\n Thá»i gian Æ°á»›c tÃ­nh CHO Má»–I SENTIMENT:')\n",
    "for label in sorted(train_df['sentiment'].unique()):\n",
    "    bt_time = augment_per_class[label] * (4 + BACKTRANS_DELAY) / 60\n",
    "    print(f'   Sentiment {label}: ~{bt_time:.1f} phÃºt')\n",
    "\n",
    "print(f'\\nMá»—i sentiment cÃ³ thá»ƒ cháº¡y riÃªng trong cÃ¡c cell khÃ¡c nhau!')\n",
    "\n",
    "# Khá»Ÿi táº¡o list Ä‘á»ƒ lÆ°u augmented data\n",
    "# QUAN TRá»ŒNG: Cháº¡y cell nÃ y TRÆ¯á»šC KHI cháº¡y cÃ¡c cell augment bÃªn dÆ°á»›i!\n",
    "augmented_train_data = []\n",
    "print(f'\\nÄÃ£ khá»Ÿi táº¡o augmented_train_data = []')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b72443",
   "metadata": {},
   "source": [
    "## 1.8.1. Augment Sentiment \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f9ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”„ Äang augment TRAIN sentiment negative\n",
      "================================================================================\n",
      "\n",
      "ğŸŒ Back Translation (10,000 samples)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2339dd995644e11943463e48aced5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BackTrans negative:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… HoÃ n thÃ nh BackTrans: 9997/10000 thÃ nh cÃ´ng trong 580.2 phÃºt\n",
      "\n",
      "âœ… ÄÃ£ augment xong sentiment negative!\n",
      "   Total augmented: 9,997 samples\n"
     ]
    }
   ],
   "source": [
    "sentiment_label = \"negative\"\n",
    "\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print(f'Äang augment TRAIN sentiment {sentiment_label}')\n",
    "print(f'{\"=\"*80}')\n",
    "\n",
    "# Láº¥y táº¥t cáº£ samples cá»§a class nÃ y tá»« TRAIN\n",
    "class_samples = train_df[train_df['sentiment'] == sentiment_label]\n",
    "\n",
    "# ===== BACK TRANSLATION AUGMENTATION =====\n",
    "num_augment = augment_per_class[sentiment_label]\n",
    "print(f'\\nBack Translation ({num_augment:,} samples)...')\n",
    "\n",
    "if num_augment > 0:\n",
    "    bt_samples = class_samples.sample(n=num_augment, replace=True, random_state=SEED)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    success_count = 0\n",
    "    \n",
    "    for idx, row in tqdm(bt_samples.iterrows(), total=num_augment, desc=f'BackTrans {sentiment_label}'):\n",
    "        original = row['review']\n",
    "        # Truncate Ä‘á»ƒ trÃ¡nh quÃ¡ dÃ i\n",
    "        truncated = original[:MAX_BACKTRANS_LENGTH] if len(original) > MAX_BACKTRANS_LENGTH else original\n",
    "        \n",
    "        back_translated = back_translator.back_translate(truncated)\n",
    "        if back_translated and back_translated != truncated:\n",
    "            augmented_train_data.append({\n",
    "                'review': back_translated,\n",
    "                'sentiment': sentiment_label,\n",
    "                'augment_method': 'BackTranslation'\n",
    "            })\n",
    "            success_count += 1\n",
    "        \n",
    "        time.sleep(BACKTRANS_DELAY)\n",
    "    \n",
    "    bt_elapsed = time.time() - start_time\n",
    "    print(f'   HoÃ n thÃ nh BackTrans: {success_count}/{num_augment} thÃ nh cÃ´ng trong {bt_elapsed/60:.1f} phÃºt')\n",
    "else:\n",
    "    print(f'   Bá» qua BackTranslation (num_augment = 0)')\n",
    "\n",
    "print(f'\\nÄÃ£ augment xong sentiment {sentiment_label}!')\n",
    "print(f'   Total augmented: {success_count:,} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5700ae6e",
   "metadata": {},
   "source": [
    "## 1.8.2. Augment Sentiment \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f1c422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”„ Äang augment TRAIN sentiment positive\n",
      "================================================================================\n",
      "\n",
      "ğŸŒ Back Translation (10,000 samples)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813e8a3ca75f4c26bfb727d6fb7a528a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BackTrans positive:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… HoÃ n thÃ nh BackTrans: 9998/10000 thÃ nh cÃ´ng trong 579.4 phÃºt\n",
      "\n",
      "âœ… ÄÃ£ augment xong sentiment positive!\n",
      "   Total augmented: 9,998 samples\n"
     ]
    }
   ],
   "source": [
    "sentiment_label = \"positive\"\n",
    "\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print(f'Äang augment TRAIN sentiment {sentiment_label}')\n",
    "print(f'{\"=\"*80}')\n",
    "\n",
    "# Láº¥y táº¥t cáº£ samples cá»§a class nÃ y tá»« TRAIN\n",
    "class_samples = train_df[train_df['sentiment'] == sentiment_label]\n",
    "\n",
    "# ===== BACK TRANSLATION AUGMENTATION =====\n",
    "num_augment = augment_per_class[sentiment_label]\n",
    "print(f'\\nBack Translation ({num_augment:,} samples)...')\n",
    "\n",
    "if num_augment > 0:\n",
    "    bt_samples = class_samples.sample(n=num_augment, replace=True, random_state=SEED)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    success_count = 0\n",
    "    \n",
    "    for idx, row in tqdm(bt_samples.iterrows(), total=num_augment, desc=f'BackTrans {sentiment_label}'):\n",
    "        original = row['review']\n",
    "        # Truncate Ä‘á»ƒ trÃ¡nh quÃ¡ dÃ i\n",
    "        truncated = original[:MAX_BACKTRANS_LENGTH] if len(original) > MAX_BACKTRANS_LENGTH else original\n",
    "        \n",
    "        back_translated = back_translator.back_translate(truncated)\n",
    "        if back_translated and back_translated != truncated:\n",
    "            augmented_train_data.append({\n",
    "                'review': back_translated,\n",
    "                'sentiment': sentiment_label,\n",
    "                'augment_method': 'BackTranslation'\n",
    "            })\n",
    "            success_count += 1\n",
    "        \n",
    "        time.sleep(BACKTRANS_DELAY)\n",
    "    \n",
    "    bt_elapsed = time.time() - start_time\n",
    "    print(f'   HoÃ n thÃ nh BackTrans: {success_count}/{num_augment} thÃ nh cÃ´ng trong {bt_elapsed/60:.1f} phÃºt')\n",
    "else:\n",
    "    print(f'   Bá» qua BackTranslation (num_augment = 0)')\n",
    "\n",
    "print(f'\\nÄÃ£ augment xong sentiment {sentiment_label}!')\n",
    "print(f'   Total augmented: {success_count:,} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfb75f0",
   "metadata": {},
   "source": [
    "## 1.8.3. Tá»•ng Káº¿t Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04dca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "âœ… ÄÃƒ HOÃ€N THÃ€NH Táº¤T Cáº¢ AUGMENTATION!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Tá»•ng káº¿t:\n",
      "   Tá»•ng augmented samples: 19,995\n",
      "\n",
      "ğŸ“Š PhÃ¢n bá»‘ theo phÆ°Æ¡ng phÃ¡p:\n",
      "   BackTranslation: 19,995 (100.0%)\n",
      "\n",
      "ğŸ“Š PhÃ¢n bá»‘ theo sentiment:\n",
      "   negative: 9,997 (50.0%)\n",
      "   positive: 9,998 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n{\"=\"*80}')\n",
    "print(f'ÄÃƒ HOÃ€N THÃ€NH Táº¤T Cáº¢ AUGMENTATION!')\n",
    "print(f'{\"=\"*80}')\n",
    "\n",
    "print(f'\\nTá»•ng káº¿t:')\n",
    "print(f'   Tá»•ng augmented samples: {len(augmented_train_data):,}')\n",
    "\n",
    "# Thá»‘ng kÃª theo method\n",
    "if len(augmented_train_data) > 0:\n",
    "    df_aug_temp = pd.DataFrame(augmented_train_data)\n",
    "    \n",
    "    print(f'\\nPhÃ¢n bá»‘ theo phÆ°Æ¡ng phÃ¡p:')\n",
    "    for method, count in df_aug_temp['augment_method'].value_counts().items():\n",
    "        pct = count / len(df_aug_temp) * 100\n",
    "        print(f'   {method}: {count:,} ({pct:.1f}%)')\n",
    "    \n",
    "    print(f'\\nPhÃ¢n bá»‘ theo sentiment:')\n",
    "    for sentiment, count in df_aug_temp['sentiment'].value_counts().sort_index().items():\n",
    "        pct = count / len(df_aug_temp) * 100\n",
    "        print(f'   {sentiment}: {count:,} ({pct:.1f}%)')\n",
    "else:\n",
    "    print(f'\\nChÆ°a cÃ³ dá»¯ liá»‡u augmented! HÃ£y cháº¡y cÃ¡c cell 1.8.1 vÃ  1.8.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a9eca1",
   "metadata": {},
   "source": [
    "## 1.9. Káº¿t Há»£p Train Gá»‘c vÃ  Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f660c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š THá»NG KÃŠ Dá»® LIá»†U SAU KHI AUGMENT\n",
      "================================================================================\n",
      "\n",
      "ğŸ”µ TRAIN SET (ÄÃƒ AUGMENT - BACK TRANSLATION):\n",
      "   Original train: 40,000 samples\n",
      "   Augmented (Back Translation): 19,995 samples\n",
      "   Total train: 59,995 samples\n",
      "\n",
      "   PhÃ¢n phá»‘i nhÃ£n train (sau augment):\n",
      "      Sentiment negative: 29,997 (50.0%)\n",
      "      Sentiment positive: 29,998 (50.0%)\n",
      "\n",
      "ğŸ”´ TEST SET (KHÃ”NG AUGMENT - NGUYÃŠN Báº¢N):\n",
      "   Total test: 10,000 samples\n",
      "\n",
      "   PhÃ¢n phá»‘i nhÃ£n test (nguyÃªn báº£n):\n",
      "      Sentiment negative: 5,000 (50.0%)\n",
      "      Sentiment positive: 5,000 (50.0%)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Táº¡o DataFrame augmented train (bá» cá»™t augment_method trÆ°á»›c khi káº¿t há»£p)\n",
    "df_augmented_train = pd.DataFrame(augmented_train_data)\n",
    "df_augmented_train = df_augmented_train[['review', 'sentiment']]  # Chá»‰ giá»¯ 2 cá»™t cáº§n thiáº¿t\n",
    "\n",
    "# Káº¿t há»£p train gá»‘c vá»›i augmented train\n",
    "train_combined = pd.concat([train_df, df_augmented_train], ignore_index=True)\n",
    "\n",
    "# Shuffle\n",
    "train_combined = train_combined.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print('='*80)\n",
    "print('THá»NG KÃŠ Dá»® LIá»†U SAU KHI AUGMENT')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\nTRAIN SET (ÄÃƒ AUGMENT - BACK TRANSLATION):')\n",
    "print(f'   Original train: {len(train_df):,} samples')\n",
    "print(f'   Augmented (Back Translation): {len(df_augmented_train):,} samples')\n",
    "print(f'   Total train: {len(train_combined):,} samples')\n",
    "print(f'\\n   PhÃ¢n phá»‘i nhÃ£n train (sau augment):')\n",
    "for label, count in train_combined['sentiment'].value_counts().sort_index().items():\n",
    "    pct = count / len(train_combined) * 100\n",
    "    print(f'      Sentiment {label}: {count:,} ({pct:.1f}%)')\n",
    "\n",
    "print(f'\\nTEST SET (KHÃ”NG AUGMENT - NGUYÃŠN Báº¢N):')\n",
    "print(f'   Total test: {len(test_df):,} samples')\n",
    "print(f'\\n   PhÃ¢n phá»‘i nhÃ£n test (nguyÃªn báº£n):')\n",
    "for label, count in test_df['sentiment'].value_counts().sort_index().items():\n",
    "    pct = count / len(test_df) * 100\n",
    "    print(f'      Sentiment {label}: {count:,} ({pct:.1f}%)')\n",
    "\n",
    "print('\\n' + '='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece380b",
   "metadata": {},
   "source": [
    "## 1.10. LÆ°u Dá»¯ Liá»‡u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37bbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Äang lÆ°u dá»¯ liá»‡u...\n",
      "\n",
      "âœ… ÄÃ£ lÆ°u TRAIN (augmented): ./split_augmented_data\\train_augmented.csv\n",
      "   Samples: 59,995\n",
      "   Size: 59.66 MB\n",
      "\n",
      "âœ… TEST (original - Ä‘Ã£ lÆ°u trÆ°á»›c Ä‘Ã³): ./split_augmented_data\\test_original.csv\n",
      "   Samples: 10,000\n",
      "   Size: 12.60 MB\n",
      "âœ… ÄÃ£ lÆ°u TRAIN (augmented): ./split_augmented_data\\train_augmented.csv\n",
      "   Samples: 59,995\n",
      "   Size: 59.66 MB\n",
      "\n",
      "âœ… TEST (original - Ä‘Ã£ lÆ°u trÆ°á»›c Ä‘Ã³): ./split_augmented_data\\test_original.csv\n",
      "   Samples: 10,000\n",
      "   Size: 12.60 MB\n"
     ]
    }
   ],
   "source": [
    "print('Äang lÆ°u dá»¯ liá»‡u...\\n')\n",
    "\n",
    "# LÆ°u train augmented\n",
    "train_path = os.path.join(OUTPUT_DIR, 'train_augmented.csv')\n",
    "train_combined.to_csv(train_path, index=False)\n",
    "print(f'ÄÃ£ lÆ°u TRAIN (augmented): {train_path}')\n",
    "print(f'   Samples: {len(train_combined):,}')\n",
    "print(f'   Size: {os.path.getsize(train_path) / 1024 / 1024:.2f} MB')\n",
    "\n",
    "# Test Ä‘Ã£ Ä‘Æ°á»£c lÆ°u á»Ÿ bÆ°á»›c 1.4\n",
    "print(f'\\nTEST (original - Ä‘Ã£ lÆ°u trÆ°á»›c Ä‘Ã³): {test_path}')\n",
    "print(f'   Samples: {len(test_df):,}')\n",
    "print(f'   Size: {os.path.getsize(test_path) / 1024 / 1024:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d787a",
   "metadata": {},
   "source": [
    "## 1.11. Tá»•ng Káº¿t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16508315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ‰ HOÃ€N THÃ€NH CHIA VÃ€ AUGMENT Dá»® LIá»†U!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Tá»•ng Káº¿t:\n",
      "   âœ… Input: ./data/dataset.csv\n",
      "   âœ… Original dataset: 50,000 samples\n",
      "   âœ… Train/Test split: 80%/20%\n",
      "   ğŸ“ Files Ä‘Ã£ táº¡o:\n",
      "      1. ./split_augmented_data\\train_augmented.csv\n",
      "         - Train gá»‘c: 40,000 samples\n",
      "         - Augmented (Back Translation): 19,995 samples\n",
      "         - Tá»•ng: 59,995 samples\n",
      "\n",
      "      2. ./split_augmented_data\\test_original.csv\n",
      "         - Test gá»‘c (KHÃ”NG augment): 10,000 samples\n",
      "\n",
      "âš ï¸  LÆ¯U Ã QUAN TRá»ŒNG:\n",
      "   âœ… Táº­p TRAIN Ä‘Ã£ Ä‘Æ°á»£c augment báº±ng Back Translation:\n",
      "      â€¢ Back Translation: Paraphrase qua ngÃ´n ngá»¯ trung gian\n",
      "   âœ… Táº­p TEST giá»¯ NGUYÃŠN - KHÃ”NG augment\n",
      "   âœ… TrÃ¡nh data leakage - Ä‘Ã¡nh giÃ¡ khÃ¡ch quan!\n",
      "\n",
      "ğŸ’¡ BÆ°á»›c tiáº¿p theo:\n",
      "   1. Cháº¡y notebook 2_encode_split_data.ipynb Ä‘á»ƒ mÃ£ hÃ³a dá»¯ liá»‡u\n",
      "   2. Cháº¡y notebook 3_train_with_split.ipynb Ä‘á»ƒ train model\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ§¹ ÄÃ£ giáº£i phÃ³ng bá»™ nhá»›\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('HOÃ€N THÃ€NH CHIA VÃ€ AUGMENT Dá»® LIá»†U!')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\nTá»•ng Káº¿t:')\n",
    "print(f'   Input: {DATA_PATH}')\n",
    "print(f'   Original dataset: {len(df):,} samples')\n",
    "print(f'   Train/Test split: {100*(1-TEST_SIZE):.0f}%/{100*TEST_SIZE:.0f}%')\n",
    "print(f'   Files Ä‘Ã£ táº¡o:')\n",
    "print(f'      1. {train_path}')\n",
    "print(f'         - Train gá»‘c: {len(train_df):,} samples')\n",
    "print(f'         - Augmented (Back Translation): {len(df_augmented_train):,} samples')\n",
    "print(f'         - Tá»•ng: {len(train_combined):,} samples')\n",
    "print(f'\\n      2. {test_path}')\n",
    "print(f'         - Test gá»‘c (KHÃ”NG augment): {len(test_df):,} samples')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "\n",
    "# Giáº£i phÃ³ng bá»™ nhá»›\n",
    "del back_translator\n",
    "print('\\nğŸ§¹ ÄÃ£ giáº£i phÃ³ng bá»™ nhá»›')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97715ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
